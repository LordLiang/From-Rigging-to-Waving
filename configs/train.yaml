# default settings pay more attention to dataset mask sample weight, batch_size, dataset name
TASK_TYPE: train_rigging2waving_entrance
guide_scale: 2.5
vit_resolution: [224, 224]
resolution: [512, 768] 
use_fp16: True
use_ema: True
max_frames: 16
latent_random_ref: True
latent_coarse_image: False
chunk_size: 2
decoder_bs: 4
num_workers: 4
sample_fps : [2, ]
frame_lens : [16, ]
scale: 8
use_fps_condition: False
train_layers: 'spatial'
train_pose: False #True


embedder: {
    'type': 'FrozenOpenCLIPTextVisualEmbedder',
    'layer': 'penultimate',
    'pretrained': 'checkpoints/open_clip_pytorch_model.bin' 
}


auto_encoder: {
    'type': 'AutoencoderKL',
    'ddconfig': {
        'double_z': True, 
        'z_channels': 4,
        'resolution': 256, 
        'in_channels': 3,
        'out_ch': 3, 
        'ch': 128, 
        'ch_mult': [1, 2, 4, 4],
        'num_res_blocks': 2, 
        'attn_resolutions': [], 
        'dropout': 0.0,
        'video_kernel_size': [3, 1, 1]
    },
    'embed_dim': 4,
    'pretrained': 'checkpoints/v2-1_512-ema-pruned.ckpt'
}

UNet: {
    'type': 'UNetSD_Rigging2Waving',
    'config': None,
    'in_dim': 4,
    'dim': 320,
    'y_dim': 1024,
    'context_dim': 1024,
    'out_dim': 4,
    'dim_mult': [1, 2, 4, 4],
    'num_heads': 8,
    'head_dim': 64,
    'num_res_blocks': 2,
    'dropout': 0.1,
    'temporal_attention': True,
    'num_tokens': 4,
    'temporal_attn_times': 1,
    'use_checkpoint': True,
    'use_fps_condition': False,
    'use_sim_mask': False,
    'latent_coarse_image': False,
}
video_compositions: ['image', 'coarse_image', 'dwpose', 'randomref', 'randomref_pose']


Diffusion: {
    'type': 'DiffusionDDIM',
    'schedule': 'linear_sd', 
    'schedule_param': {
        'num_timesteps': 1000,
        "init_beta": 0.00085, 
        "last_beta": 0.0120,
        'zero_terminal_snr': True,
    },
    'mean_type': 'v',
    'loss_type': 'mse',
    'var_type': 'fixed_small', # 'fixed_large',
    'rescale_timesteps': False,
    'noise_strength': 0.1
}
use_DiffusionDPM: False
CPU_CLIP_VAE: True
noise_prior_value: 949 # or 999, 949



vid_dataset: {
    'type': 'VideoDataset',
    'data_list': 'data/cartoon_dataset_v3_train',
    'vit_resolution': [224, 224],
    'resolution': [512, 768],
    'kernel_size': 15,
    'iters': [8,15],
    'weights': [1,4,1],
    'turn_background_mask': True,
}


batch_sizes: {
    "1": 1,
    "4": 1,
    "8": 1,
    "16": 1,
    "24": 1,
    "32": 1
}

visual_train: {
    'type': 'VisualTrainImageToVideo',
    'partial_keys': [
        ['image', 'dwpose', 'coarse_image','randomref']
    ],
    'use_offset_noise': True,
    'guide_scale': 9.0, 
}


Pretrain: {
    'type': pretrain_specific_strategies,
    'fix_weight': False,
    'grad_scale': 0.5,
    'resume_checkpoint': 'checkpoints/unianimate_16f_32f_non_ema_223000.pth',
    'sd_keys_path': 'pre_trainedmodels/stable_diffusion_image_key_temporal_attention_x1.json',
}
resume_from_zero: True


lr: 0.00002
weight_decay: 1.0e-2
use_fsdp: False
#use_loss_mask: False
#w_loss_mask: 1.0


noise_strength: 0.1
# classifier-free guidance
p_zero: 0.0
guide_scale: 2.5
ddim_timesteps: 20  # among 25-50
num_steps: 50005
ema_start: 49999

use_zero_infer: True
viz_interval: 2   # 20000
save_ckp_interval: 10000   # 500

# Log
log_dir: "workspace/experiments"
log_interval: 1
seed: 6666


#Lora
train_lora: True
use_lora: True
lora_r: 32
lora_alpha: 32
#lora_dropout: 0.05
